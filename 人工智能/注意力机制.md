## _注意力汇聚_（attention pooling）公式
用数学语言描述，假设有一个查询 $\mathbf{q} \in \mathbb{R}^q$ 和 $m$ 个“键一值”对 $\left(\mathbf{k}_1, \mathbf{v}_1\right), \ldots,\left(\mathbf{k}_m, \mathbf{v}_m\right)$, 其中 $\mathbf{k}_i \in \mathbb{R}^k, \mathbf{v}_i \in \mathbb{R}^v$ 。注意力汇聚函数 $f$ 就被表示成值的加权和:
$$
f\left(\mathbf{q},\left(\mathbf{k}_1, \mathbf{v}_1\right), \ldots,\left(\mathbf{k}_m, \mathbf{v}_m\right)\right)=\sum_{i=1}^m \alpha\left(\mathbf{q}, \mathbf{k}_i\right) \mathbf{v}_i \in \mathbb{R}^v
$$

其中查询 $\mathbf{q}$ 和键 $\mathbf{k}_i$ 的注意力权重（标量） 是通过注意力评分函数 $a$ 将两个向量映射成标量, 再经过 softmax运算得到的:
$$
\alpha\left(\mathbf{q}, \mathbf{k}_i\right)=\operatorname{softmax}\left(a\left(\mathbf{q}, \mathbf{k}_i\right)\right)=\frac{\exp \left(a\left(\mathbf{q}, \mathbf{k}_i\right)\right)}{\sum_{j=1}^m \exp \left(a\left(\mathbf{q}, \mathbf{k}_j\right)\right)} \in \mathbb{R} .
$$

正如上图所示, 选择不同的注意力评分函数 $a$ 会导致不同的注意力汇聚操作。

## 几种评分函数
### Nadaraya-Watson 核回归
$$
\text{score}(Q, K) = K_h(Q - K)
$$
注意力机制通常是参数化的，这意味着它有学习的参数，如权重矩阵，这些参数在训练过程中会被调整。而 Nadaraya-Watson 核回归是非参数化的，它没有需要学习的参数，但需要选择合适的核函数和带宽。
### 缩放点积注意力 (Scaled Dot-Product Attention):

- **计算方式**：此方法首先计算查询 \( Q \) 和键 \( K \) 之间的点积，然后将结果除以一个缩放因子（通常是键的维度的平方根）。
  $$
  \text{score}(Q, K) = \frac{Q \cdot K^T}{\sqrt{d_k}}
  $$
  
- **效率**：由于它涉及的主要是矩阵乘法操作，所以在硬件加速器（如 GPU）上可以高效地并行计算。
- **适用情况**：当查询和键的维度较大时，缩放点积注意力特别有用，因为缩放可以帮助保持权重的稳定性。

### 加性注意力 (Additive Attention):

- **计算方式**：此方法计算查询和键之间的差异，然后将该差异通过一个前馈网络（通常只有一层）来计算得分。
  $$
  \text{score}(Q, K) = v^T \cdot \text{tanh}(W_1 \cdot Q + W_2 \cdot K)
 $$
  
  其中，$v$、$W_1$ 和 $W_2$ 是需要学习的参数。
- **效率**：与缩放点积注意力相比，加性注意力在计算上通常更为复杂，因为它涉及更多的参数和非线性操作。
- **适用情况**：在查询和键的维度不匹配或较小的情况下，加性注意力可能更为有效。
## 多头注意力

![[多头注意力.png]]
在实现多头注意力之前, 让我们用数学语言将这个模型形式化地描述出来。给定查询 $\mathbf{q} \in \mathbb{R}^{d_q}$ 、键 $\mathbf{k} \in \mathbb{R}^{d_k}$ 和 值 $\mathbf{v} \in \mathbb{R}^{d_v}$ , 每个注意力头 $\mathbf{h}_i(i=1, \ldots, h)$ 的计算方法为:
$$
\mathbf{h}_i=f\left(\mathbf{W}_i^{(q)} \mathbf{q}, \mathbf{W}_i^{(k)} \mathbf{k}, \mathbf{W}_i^{(v)} \mathbf{v}\right) \in \mathbb{R}^{p_v}
$$

其中, 可学习的参数包括 $\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q} 、 \mathbf{W}_i^{(k)} \in \mathbb{R}^{p_k \times d_k}$ 和 $\mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d_v}$, 以及代表注意力汇聚的函数 $f \circ f$ 可以是 10.3 节中的 加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换, 它对应着 $h$ 个头连结后的结果, 因此其可学习参数是 $\mathbf{W}_o \in \mathbb{R}^{p_o \times h p_v}$ :
$$
\mathbf{W}_o\left[\begin{array}{c}
\mathbf{h}_1 \\
\vdots \\
\mathbf{h}_h
\end{array}\right] \in \mathbb{R}^{p_o} .
$$

基于这种设计, 每个头都可能会关注输入的不同部分, 可以表示比简单加权平均值更复杂的函数。
## 自注意力、多头自注意力、位置编码

![[自注意力.png]]
https://0809zheng.github.io/2020/04/24/self-attention.html