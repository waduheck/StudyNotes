## Nadaraya-Watson 核回归

## 1. 缩放点积注意力 (Scaled Dot-Product Attention):

- **计算方式**：此方法首先计算查询 \( Q \) 和键 \( K \) 之间的点积，然后将结果除以一个缩放因子（通常是键的维度的平方根）。
  $$
  \text{score}(Q, K) = \frac{Q \cdot K^T}{\sqrt{d_k}}
  $$
  
- **效率**：由于它涉及的主要是矩阵乘法操作，所以在硬件加速器（如 GPU）上可以高效地并行计算。
- **适用情况**：当查询和键的维度较大时，缩放点积注意力特别有用，因为缩放可以帮助保持权重的稳定性。

## 2. 加性注意力 (Additive Attention):

- **计算方式**：此方法计算查询和键之间的差异，然后将该差异通过一个前馈网络（通常只有一层）来计算得分。
  $$
  \text{score}(Q, K) = v^T \cdot \text{tanh}(W_1 \cdot Q + W_2 \cdot K)
 $$
  
  其中，$v$、$W_1$ 和 $W_2$ 是需要学习的参数。
- **效率**：与缩放点积注意力相比，加性注意力在计算上通常更为复杂，因为它涉及更多的参数和非线性操作。
- **适用情况**：在查询和键的维度不匹配或较小的情况下，加性注意力可能更为有效。
 