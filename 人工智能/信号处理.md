**“时域的相乘等于频域的卷积，时域的卷积等于频域相乘”**
## 阶跃函数、冲激函数
https://blog.csdn.net/qq_44431690/article/details/104867804
### 狄拉克梳状函数
将梳状函数与信号函数进行卷积即进行采样
采样后的傅立叶展开等于将信号函数的频域进行复制

## 傅立叶变换
https://tracholar.github.io/math/2017/03/12/fourier-transform.html
精彩的解析，从各个角度解释傅立叶变换的特殊性
### 一维
https://kaizhao.net/blog/fourier
详细的数学推导
https://www.bilibili.com/video/BV1pW411J7s8/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=30e06232c8ac3c33f2c9dda1c3ffd025
3b1b的视频
### 二维
https://www.bilibili.com/video/BV1f7411r7tb/?spm_id_from=333.337.search-card.all.click&vd_source=30e06232c8ac3c33f2c9dda1c3ffd025
形象展示的视频
https://www.zhihu.com/question/22611929
科普文

### k空间

这个空间中的点 $\mathrm{Q}(u, v)$ 代表这个平面波的传播方向 $\vec{n}$, 这个向量的模 $\sqrt{u^2+v^2}$ 代表这个平面波的频率 $w$, 这个点里面保存的内容复数就是此平面波的幅度和相位。


## 采样定理
https://zhuanlan.zhihu.com/p/74736706
将梳状函数与信号函数进行卷积即进行采样
采样后的傅立叶展开等于将信号函数的频域进行复制
![[采样.jpeg]]

![[混叠.webp]]


## 小波变换
## Hann窗口
Hann窗口（有时也被称为Hanning窗口，但Hann更为正式）是信号处理中经常使用的窗函数之一。窗函数的主要目的是将无限的信号乘以一个有限的函数，以使其在某个区域内为非零，并在区域外为零。这样做可以帮助减少信号转换（如傅里叶变换）时的边缘效应。

Hann窗口的定义为：

$w(n) = 0.5 \times (1 - \cos(2\pi n / (N-1)))$

其中：
- \( n \) 是当前的样本或位置。
- \( N \) 是窗口的长度。

当应用Hann窗口时，窗口的中心部分接近1，而在两端逐渐接近0，形成一个光滑的钟形曲线。由于其特性，Hann窗口可以减少信号的频谱泄露。

至于"sliding a truncated Hann window"，这指的是在信号上移动或"滑动"一个被截断的Hann窗口。被截断意味着只使用Hann窗口的一部分。在某些应用中，可能只需要窗口的一部分来对信号进行局部加权。滑动窗口通常用于时间序列分析，其中一个窗口在时间上移动，以捕捉信号的局部特性。

## 位置编码
https://zhuanlan.zhihu.com/p/454482273

***不仅可以储存绝对位置，还可以表示相对位置。***

那现在我们对位置向量再提出一个要求, 不同的位置向量是可以通过线性转换得到的。这样, 我们不仅能表示一个token的绝对位置, 还可以表示一个token的相对位置, 即我们想要：
$$
P E_{t+\triangle t}=T_{\triangle t} * P E_t
$$

这里, T表示一个线性变换矩阵。观察这个目标式子, 联想到在向量空间中一种常用的线形变换一旋转。在这里, 我们将想象为一个角度, 那么 $\triangle t$ 就是其旋转的角度, 则上面的式子可以进一步写成:
$$
\left(\begin{array}{c}
\sin (t+\triangle t) \\
\cos ((t+\triangle t)
\end{array}\right)=\left(\begin{array}{cc}
\cos \triangle t & \sin \triangle t \\
-\sin \triangle t & \cos \triangle t
\end{array}\right)\left(\begin{array}{c}
\sin t \\
\cos t
\end{array}\right)
$$
## 归一化
当然可以。我们来详细探讨层归一化和批归一化，并通过例子来理解它们的不同之处。

### 批归一化 (Batch Normalization, BatchNorm)

#### 工作原理：

- 对于给定的特征，BatchNorm 在一个批次中的所有样本上计算均值和方差。
- 使用这些均值和方差来归一化每个样本的该特征。
- 使用可学习的参数 \( \gamma \) 和 \( \beta \) 对归一化的特征进行缩放和偏移。

#### 例子：

假设我们有一个批次的数据，包含3个样本，每个样本有2个特征：
$$
\text{Batch} = 
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
\end{bmatrix}
$$

对于第一个特征，均值是 $(1 + 3 + 5) / 3 = 3$，方差是 $((1-3)^2 + (3-3)^2 + (5-3)^2) / 3$。

对于第二个特征，我们也会计算相应的均值和方差。

然后，我们使用这些均值和方差来归一化整个批次的数据。

### 层归一化 (Layer Normalization, LayerNorm)

#### 工作原理：

- 对于给定的样本，LayerNorm 在所有特征上计算均值和方差。
- 使用这些均值和方差来归一化该样本的所有特征。
- 使用可学习的参数 \( \gamma \) 和 \( \beta \) 对归一化的特征进行缩放和偏移。

#### 例子：

使用上述相同的数据批次：

$$
\text{Batch} = 
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
\end{bmatrix}
$$

对于第一个样本，均值是 $(1 + 2) / 2 = 1.5$，方差是 $((1-1.5)^2 + (2-1.5)^2) / 2$。

然后，我们使用这些均值和方差来归一化第一个样本的所有特征。

对于第二和第三个样本，我们也会进行类似的计算。

### 区别的核心：

- 在**BatchNorm**中，每个特征的归一化是基于整个批次的所有样本进行的。
- 在**LayerNorm**中，每个样本的归一化是基于该样本的所有特征进行的。

这意味着在 BatchNorm 中，同一特征在不同样本之间的均值和方差是固定的，而在 LayerNorm 中，同一样本的不同特征的均值和方差是固定的。

这两种归一化方法都有其优点：BatchNorm 在卷积神经网络中工作得很好，因为特征的统计数据在不同的图像之间是相似的。而 LayerNorm 在处理序列数据时特别有用，因为序列的长度和批次大小可能会有所变化，而这对 BatchNorm 影响较大。
## 滑动窗口

## PCA（主成分分析）
https://zhuanlan.zhihu.com/p/77151308
目标：面对纬度过大且稀疏的数据时，可以舍弃掉一些维度，提高样本密度。而且在大多数情况下特征值小的维度意味着噪声

于是我们取出让协方差矩阵对角化的矩阵的前k行，即取出协方差矩阵的前k个特征值对应的特征向量。做为新的空间向量基底
## 非极大值抑制算法

