## 传统新视角合成

最初的新视角合成方法基于光场技术，从密集采样开始逐渐发展到支持非结构化捕获。随着运动结构恢复（SfM）技术的出现，使用一系列照片合成新视角成为可能。SfM在相机校准期间估计稀疏点云，最初被应用于3D空间的简单可视化。紧接着，多视图立体视觉（MVS）的发展随着时间推进，产生了令人印象深刻的完整3D重建算法，进而推动了多种视角合成算法的发展。这些方法通过将输入图像重新投影并融合到新视角相机中，并利用几何信息来指导这一过程。尽管在许多情况下取得了出色的结果，这些方法通常无法完全恢复未重建区域或“过度重建”——即MVS生成了实际不存在的几何结构。最新的神经渲染算法大幅减少了这类伪影，并避免了在GPU上存储所有输入图像的巨大成本，从而在多方面超越了传统方法。

## AI新视角合成

早期的AI新视角合成技术采用深度学习方法，如卷积神经网络（CNN），用于估算融合权重或解决纹理空间问题。然而，这些方法主要依赖于基于多视图立体（MVS）的几何，常导致渲染时出现时间闪烁问题。后来，为新视角合成引入了体积表示法，通过构建连续的可微分密度场来表示几何结构。然而，尽管神经辐射场（NeRF）等方法在提高质量方面取得了成效，它们庞大的多层感知机（MLP）结构却导致了速度上的损失。最新的方法着重于通过使用空间数据结构、不同的编码方式和优化MLP容量来加快训练和/或渲染速度。尽管这些方法提供了杰出的结果，但在有效表示空间方面仍存在挑战，并且图像质量很大程度上受到用于加速的结构化网格的限制。与此相对，Gaussian Splatting采用了非结构化、显式的GPU友好的3D高斯方法，实现了更快的渲染速度和更好的质量，而无需依赖于神经网络组件。
  
神经辐射场（NeRF）的主要贡献包括：

nerf继承了体积表示法的优势：两者都能够表示复杂的现实世界几何形态和外观，并且非常适合使用投影图像进行基于梯度的优化。关键的是，克服了在高分辨率下模拟复杂场景时，离散体素网格带来的禁止性存储成本。
- 一种以基本的多层感知机（MLP）网络参数化，用于表示具有复杂几何形态和材质的连续场景的5D神经辐射场的方法。
- 一种基于经典体积渲染技术的可微分渲染过程，使用它从标准RGB图像中优化这些表示。这包括一种分层采样策略，用于将MLP的容量分配到具有可见场景内容的空间。
- 一种位置编码，将每个输入的5D坐标映射到更高维度的空间，这使能够成功优化神经辐射场以表示高频场景内容。

神经辐射场方法在定量和定性方面都超越了最先进的视角合成方法，包括那些将神经3D表示拟合到场景的工作，以及那些训练深度卷积网络来预测采样体积表示的工作。据所知，这篇论文呈现的是第一个能够从在自然设置下捕获的RGB图像中渲染出高分辨率逼真的新视角的连续神经场景表示方法。

Gaussian Splatting的主要贡献包括：

1. 引入各向异性3D高斯作为辐射场的高质量、非结构化表示方法。
2. 提出了一种优化3D高斯属性的方法，结合自适应密度控制，以创造高质量的场景捕获表示。
3. 开发了一种快速且可微分的GPU渲染方法，这种方法具有可见性感知能力、支持各向异性渲染，并能快速进行反向传播，实现了高质量的新视角合成。在先前发布的数据集上的结果表明，通过多视角捕获优化的3D高斯，可以达到等同或超过最佳隐式辐射场方法的质量。重要的是，Gaussian Splatting还实现了与最快方法相似的训练速度和质量，并且首次提供了实时渲染高质量新视角合成的方法。

## 单视角图片重建模型——zero123
近期，二维生成模型如DALL-E、Imagen和Stable Diffusion，以及视觉-语言模型如CLIP，通过在大规模图像数据集上的预训练，已经掌握了丰富的视觉概念。这些模型激发了一系列利用二维先验模型辅助三维生成任务的研究。例如，诸如DreamField、DreamFusion和Magic3D的项目遵循了针对每个形状进行优化的策略，通常对一个三维表示（如NeRF、网格或SMPL人体模型）进行优化，并利用可微分渲染技术从不同视角生成二维图像。这些生成的图像随后输入到CLIP模型或二维扩散模型中，以计算损失函数，指导三维形状的优化。此外，有研究在训练三维生成模型时利用了CLIP的嵌入空间，而其他一些研究则专注于使用二维模型的先验来生成输入网格的纹理或材质。在CLIP和大规模二维扩散模型出现之前，人们通常是从三维合成数据或真实扫描中学习三维先验。与二维图像不同，三维数据可以采用多种格式表示，目前已提出了许多专门用于表示的三维生成模型。最近，越来越多的研究集中在从单个图像中学习生成三维隐式场。有些最新的研究利用二维扩散模型进行单个形状的优化，使得文本到三维任务成为可能。为了从单张图像生成三维模型，一些研究利用文本反演来找到与输入图像最匹配的文本嵌入，然后将其送入扩散模型。此外，OpenAI开发了一个原生的三维扩散模型Point-E，用于生成点云，并最近发布了另一个模型Shap-E，旨在生成隐函数的参数，用于生成带纹理的网格或神经辐射场。

重建单视角的三维物体是一个极具挑战性的问题，需要强大的先验知识。一些研究基于三维基元集合（如网格、体素或点云）构建先验知识，并使用图像编码器进行条件化处理。这些模型因所用三维数据集的多样性而受到限制，并且由于它们的全局条件化特性，展现出较差的泛化能力。此外，它们需要额外的姿态估计步骤，以确保估计的形状与输入图像对齐。而局部条件化模型则直接使用局部图像特征进行场景重建，并显示出更强的跨域泛化能力，虽然这通常局限于近景重建。最近，MCC模型学习了从RGB-D视图进行三维重建的通用表示，并在以物体为中心的大规模视频数据集上进行了训练。zero123的工作中展示了可以直接从预训练的稳定扩散模型中提取丰富的几何信息，从而减少了对额外深度信息的需求。

zero123的主要贡献在于证明了大型扩散模型已经学习到了关于视觉世界的丰富三维先验知识，尽管它们只在二维图像上进行了训练。该研究还展示了从单个RGB图像进行新视角合成和零样本三维物体重建方面的最新进展。文章首先简要回顾了相关工作，然后描述了通过微调大型扩散模型以学习相机外参控制的方法。最后，通过一系列定量和定性实验，评估了从单张图像进行零样本视角合成和三维几何与外观重建的效果。
## text to gaussian——gaussian diffusion
图像到3D生成：大量的图像数据为单图像到3D内容生成任务提供了巨大潜力。同时，2D扩散模型的成熟给单图像3D生成带来了新机遇。然而，从单个图像生成3D内容限制了计算机的生成和想象能力，因此，采用文本到3D的方法更符合人机交互模型。

文本到3D生成：Dreamfusion首创了利用分数蒸馏采样（SDS）从固定2D扩散模型中学习3D场景。3DFuse作为工作的基准，在多个阶段引导3D模型生成方向，从粗糙到精细。TANGO根据文本提示以逼真的方式转换给定3D形状的外观风格。但这种方法需要一个3D模型作为输入。最新的文本到3D工作产生了从给定文本提示中得到的逼真、多视角一致的物体几何和颜色，遗憾的是，基于NeRF的生成耗时且无法满足工业需求。

- 首次提出基于高斯喷溅渲染管线和分数函数、朗之万动力学扩散模型的文本到3D框架。GaussianDiffusion显著加速了渲染过程，并能在文本到3D任务中产生目前最逼真的外观。
- 首次引入从不同视角的结构化噪声，旨在解决保持多视角几何一致性的挑战，如通过噪声注入方法解决多面体结构问题。此外，提出了一种基于高斯喷溅的有效噪声生成方法。
- 为解决精确的高斯图形建模与在多视角下2D扩散模型观察到的不稳定性之间的固有矛盾，引入了变分高斯喷溅模型，以减少3D高斯模型收敛到局部最小值的风险，这可能导致浮动物、毛刺或过度增生元素等伪影。

## 3d editting——gaussian editor
编辑神经场由于其形状和外观之间的复杂相互作用而具有挑战性。EditNeRF是这一领域的先驱作品，它通过对潜在代码进行条件化来编辑神经场的形状和颜色。此外，一些研究利用CLIP模型通过文本提示或参考图像来促进编辑。另一方面的研究聚焦于使用预定义的模板模型或骨架来支持特定类别内的重新定位或重新渲染。基于几何的方法将神经场转换为网格，并与隐式场同步网格变形。此外，3D编辑技术涉及将2D图像操作（如修复）与神经场训练结合起来。同时进行的工作利用静态2D和3D遮罩来限制NeRF的编辑区域。然而，这些方法有其局限性，因为3D模型的训练是一个动态过程，静态遮罩无法有效地限制它。相比之下，的研究采用高斯语义追踪在整个训练过程中追踪目标高斯。

GaussianEditor提供了快速、可控且多功能的3D编辑。一次编辑会话通常只需5-10分钟，比以前的编辑过程快得多。贡献可以总结为四个方面：
1. 引入了高斯语义追踪，实现了更详细有效的编辑控制。
2. 提出了层次化高斯喷溅（HGS），一种新的GS表示方法，在高度随机的生成引导下能更稳定地收敛到精细结果。
3. 专门设计了一种用于高斯喷溅的3D修复算法，可以快速移除和添加物体。
4. 广泛的实验表明，的方法在有效性、速度和可控性方面超过了以往的3D编辑方法。

## 动态3维重建
神经渲染技术结合了机器学习和几何推理，已成为从稀疏图像集合合成场景新视角的最有前景的方法之一。其中，神经辐射场（NeRF）尤为突出，它训练深度网络将5D输入坐标（代表空间位置和观察方向）映射为体积密度和视角依赖的辐射亮度。然而，尽管NeRF在生成图像的真实感上达到了前所未有的水平，但它只适用于静态场景。在这篇论文中，介绍了D-NeRF，这是一种将神经辐射场扩展到动态领域的方法，允许从围绕场景移动的单个摄像机重建和渲染对象在刚性和非刚性运动下的新图像。为此，将时间作为系统的额外输入，并将学习过程分为两个主要阶段：一个编码场景到规范空间，另一个将这个规范表示映射到特定时间的变形场景。这两种映射使用全连接网络同时学习。一旦网络训练完成，D-NeRF可以渲染新图像，控制相机视角和时间变量，从而控制物体运动。在刚性、关节和非刚性运动下的场景中证明了方法的有效性。代码、模型权重和动态场景数据集将发布。
为许多应用（如增强现实、虚拟现实、3D内容制作、游戏和电影产业）从稀疏的图像集合合成逼真的新视角是必要的。近期在神经渲染领域的进展，通过学习同时编码几何和外观的场景表示，已经大大超越了传统的结构从运动、光场摄影和基于图像的渲染方法。例如，神经辐射场（NeRF）表明简单的多层感知机网络可以编码从5D输入（表示空间位置和相机视角）到辐射值和体积密度的映射，从而实现极其逼真的自由视点渲染。然而，所有这些方法都假设场景是静态的，没有移动物体。在本文中，打破这个假设，提出了第一个适用于动态场景的端到端神经渲染系统。的方法将系统输入表示为一个连续的6D函数，包括3D位置、相机视角和时间成分。的观察是，物体可以移动和变形，但通常不会出现或消失。的方法，动态神经辐射场（D-NeRF），分解学习为两个模块，一个学习空间映射，另一个回归场景辐射和体积密度。学习的模型允许合成新图像，同时控制相机视角和时间成分，或者等效地，场景的动态状态。在不同类型的变形场景上进行了全面评估，结果表明D-NeRF能够在控制相机视角和时间成分的同时渲染高质量图像。的方法还能产生捕捉时变几何形状的完整3D网格。
