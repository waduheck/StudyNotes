##   
传统新视角合成

最初的新视角合成方法基于光场技术，从密集采样开始逐渐发展到支持非结构化捕获。随着运动结构恢复（SfM）技术的出现，使用一系列照片合成新视角成为可能。SfM在相机校准期间估计稀疏点云，最初被应用于3D空间的简单可视化。紧接着，多视图立体视觉（MVS）的发展随着时间推进，产生了令人印象深刻的完整3D重建算法，进而推动了多种视角合成算法的发展。这些方法通过将输入图像重新投影并融合到新视角相机中，并利用几何信息来指导这一过程。尽管在许多情况下取得了出色的结果，这些方法通常无法完全恢复未重建区域或“过度重建”——即MVS生成了实际不存在的几何结构。最新的神经渲染算法大幅减少了这类伪影，并避免了在GPU上存储所有输入图像的巨大成本，从而在多方面超越了传统方法。

## AI新视角合成

早期的AI新视角合成技术采用深度学习方法，如卷积神经网络（CNN），用于估算融合权重或解决纹理空间问题。然而，这些方法主要依赖于基于多视图立体（MVS）的几何，常导致渲染时出现时间闪烁问题。后来，为新视角合成引入了体积表示法，通过构建连续的可微分密度场来表示几何结构。然而，尽管神经辐射场（NeRF）等方法在提高质量方面取得了成效，它们庞大的多层感知机（MLP）结构却导致了速度上的损失。最新的方法着重于通过使用空间数据结构、不同的编码方式和优化MLP容量来加快训练和/或渲染速度。尽管这些方法提供了杰出的结果，但在有效表示空间方面仍存在挑战，并且图像质量很大程度上受到用于加速的结构化网格的限制。与此相对，Gaussian Splatting采用了非结构化、显式的GPU友好的3D高斯方法，实现了更快的渲染速度和更好的质量，而无需依赖于神经网络组件。

Gaussian Splatting的主要贡献包括：

1. 引入各向异性3D高斯作为辐射场的高质量、非结构化表示方法。
2. 提出了一种优化3D高斯属性的方法，结合自适应密度控制，以创造高质量的场景捕获表示。
3. 开发了一种快速且可微分的GPU渲染方法，这种方法具有可见性感知能力、支持各向异性渲染，并能快速进行反向传播，实现了高质量的新视角合成。在先前发布的数据集上的结果表明，通过多视角捕获优化的3D高斯，可以达到等同或超过最佳隐式辐射场方法的质量。重要的是，Gaussian Splatting还实现了与最快方法相似的训练速度和质量，并且首次提供了实时渲染高质量新视角合成的方法。
## 单视角图片重建模型——zero123
最近，2D生成模型（如DALL-E、Imagen和Stable Diffusion）和视觉-语言模型（如CLIP）通过在大规模图像数据集上的预训练，学习了广泛的视觉概念。这些模型激发了一系列研究使用2D先验模型来辅助3D生成任务。一些工作（如DreamField、DreamFusion和Magic3D）遵循每个形状优化的范式，通常优化一个3D表示（如NeRF、网格、SMPL人体模型），并使用可微分渲染从不同视角生成2D图像。这些图像随后被送入CLIP模型或2D扩散模型来计算损失函数，用于指导3D形状优化。此外，一些工作在训练3D生成模型时利用CLIP的嵌入空间，一些工作则专注于使用2D模型的先验来生成输入网格的纹理或材料。在CLIP和大规模2D扩散模型出现之前，人们常从3D合成数据或真实扫描中学习3D先验。与2D图像不同，3D数据可以用不同格式表示，并且已经提出了许多特定于表示的3D生成模型。最近，越来越多的工作集中在从单个图像学习生成3D隐式场。一些最新的工作利用2D扩散模型进行每个形状的优化，使得文本到3D任务成为可能。为了从单个图像生成3D模型，一些工作利用文本反演来找到输入图像最匹配的文本嵌入，然后送入扩散模型。此外，OpenAI训练了一个3D原生扩散模型Point-E，用于生成点云，他们最近发布了另一个模型Shap-E，旨在生成隐函数的参数，用于生产纹理网格或神经辐射场。
重构单视角的3D物体是一个高度挑战性的问题，需要强大的先验知识。一些研究基于3D基元集合（如网格、体素或点云）构建先验知识，并使用图像编码器进行条件化。这些模型受到所用3D数据集多样性的限制，因其全局条件化特性而展现出较差的泛化能力。此外，它们需要额外的姿态估计步骤以确保估计的形状与输入对齐。另一方面，局部条件化模型直接使用局部图像特征进行场景重建，并展示出更强的跨域泛化能力，尽管通常限于近视图重构。最近，MCC学习了从RGB-D视图进行3D重建的通用表示，并在以物体为中心的大规模视频数据集上进行训练。在zero123的工作中，展示了可以直接从预训练的稳定扩散模型中提取丰富的几何信息，减少了对额外深度信息的需求。

zero123的主要贡献在于证明大型扩散模型已经学习了关于视觉世界的丰富3D先验知识，即使它们只在2D图像上进行训练。还展示了在从单个RGB图像进行新视角合成和零样本3D物体重建方面的最新成果。文中首先简要回顾了相关工作，然后描述了通过微调大型扩散模型来学习相机外参控制的方法。最后，展示了多项定量和定性实验来评估从单张图像进行零样本视角合成和3D几何与外观重建。将发布所有代码、模型及在线演示。

## text to gaussian——gaussian diffusion
图像到3D生成：大量的图像数据为单图像到3D内容生成任务提供了巨大潜力。同时，2D扩散模型的成熟给单图像3D生成带来了新机遇。然而，从单个图像生成3D内容限制了计算机的生成和想象能力，因此，采用文本到3D的方法更符合人机交互模型。

文本到3D生成：Dreamfusion首创了利用分数蒸馏采样（SDS）从固定2D扩散模型中学习3D场景。3DFuse作为我们工作的基准，在多个阶段引导3D模型生成方向，从粗糙到精细。TANGO根据文本提示以逼真的方式转换给定3D形状的外观风格。但这种方法需要一个3D模型作为输入。最新的文本到3D工作产生了从给定文本提示中得到的逼真、多视角一致的物体几何和颜色，遗憾的是，基于NeRF的生成耗时且无法满足工业需求。

- 首次提出基于高斯喷溅渲染管线和分数函数、朗之万动力学扩散模型的文本到3D框架。GaussianDiffusion显著加速了渲染过程，并能在文本到3D任务中产生目前最逼真的外观。
- 首次引入从不同视角的结构化噪声，旨在解决保持多视角几何一致性的挑战，如通过噪声注入方法解决多面体结构问题。此外，我们提出了一种基于高斯喷溅的有效噪声生成方法。
- 为解决精确的高斯图形建模与在多视角下2D扩散模型观察到的不稳定性之间的固有矛盾，我们引入了变分高斯喷溅模型，以减少3D高斯模型收敛到局部最小值的风险，这可能导致浮动物、毛刺或过度增生元素等伪影。

## 3d editting——gaussian editor
编辑神经场由于其形状和外观之间的复杂相互作用而具有挑战性。EditNeRF是这一领域的先驱作品，它通过对潜在代码进行条件化来编辑神经场的形状和颜色。此外，一些研究利用CLIP模型通过文本提示或参考图像来促进编辑。另一方面的研究聚焦于使用预定义的模板模型或骨架来支持特定类别内的重新定位或重新渲染。基于几何的方法将神经场转换为网格，并与隐式场同步网格变形。此外，3D编辑技术涉及将2D图像操作（如修复）与神经场训练结合起来。同时进行的工作利用静态2D和3D遮罩来限制NeRF的编辑区域。然而，这些方法有其局限性，因为3D模型的训练是一个动态过程，静态遮罩无法有效地限制它。相比之下，我们的研究采用高斯语义追踪在整个训练过程中追踪目标高斯。

GaussianEditor提供了快速、可控且多功能的3D编辑。一次编辑会话通常只需5-10分钟，比以前的编辑过程快得多。贡献可以总结为四个方面：
1. 我们引入了高斯语义追踪，实现了更详细有效的编辑控制。
2. 我们提出了层次化高斯喷溅（HGS），一种新的GS表示方法，在高度随机的生成引导下能更稳定地收敛到精细结果。
3. 我们专门设计了一种用于高斯喷溅的3D修复算法，可以快速移除和添加物体。
4. 广泛的实验表明，我们的方法在有效性、速度和可控性方面超过了以往的3D编辑方法。

## 动态3维重建
神经渲染技术结合了机器学习和几何推理，已成为从稀疏图像集合合成场景新视角的最有前景的方法之一。其中，神经辐射场（NeRF）尤为突出，它训练深度网络将5D输入坐标（代表空间位置和观察方向）映射为体积密度和视角依赖的辐射亮度。然而，尽管NeRF在生成图像的真实感上达到了前所未有的水平，但它只适用于静态场景。在这篇论文中，介绍了D-NeRF，这是一种将神经辐射场扩展到动态领域的方法，允许从围绕场景移动的单个摄像机重建和渲染对象在刚性和非刚性运动下的新图像。为此，将时间作为系统的额外输入，并将学习过程分为两个主要阶段：一个编码场景到规范空间，另一个将这个规范表示映射到特定时间的变形场景。这两种映射使用全连接网络同时学习。一旦网络训练完成，D-NeRF可以渲染新图像，控制相机视角和时间变量，从而控制物体运动。在刚性、关节和非刚性运动下的场景中证明了方法的有效性。代码、模型权重和动态场景数据集将发布。
为许多应用（如增强现实、虚拟现实、3D内容制作、游戏和电影产业）从稀疏的图像集合合成逼真的新视角是必要的。近期在神经渲染领域的进展，通过学习同时编码几何和外观的场景表示，已经大大超越了传统的结构从运动、光场摄影和基于图像的渲染方法。例如，神经辐射场（NeRF）表明简单的多层感知机网络可以编码从5D输入（表示空间位置和相机视角）到辐射值和体积密度的映射，从而实现极其逼真的自由视点渲染。然而，所有这些方法都假设场景是静态的，没有移动物体。在本文中，打破这个假设，提出了第一个适用于动态场景的端到端神经渲染系统。的方法将系统输入表示为一个连续的6D函数，包括3D位置、相机视角和时间成分。的观察是，物体可以移动和变形，但通常不会出现或消失。的方法，动态神经辐射场（D-NeRF），分解学习为两个模块，一个学习空间映射，另一个回归场景辐射和体积密度。学习的模型允许合成新图像，同时控制相机视角和时间成分，或者等效地，场景的动态状态。在不同类型的变形场景上进行了全面评估，结果表明D-NeRF能够在控制相机视角和时间成分的同时渲染高质量图像。的方法还能产生捕捉时变几何形状的完整3D网格。
