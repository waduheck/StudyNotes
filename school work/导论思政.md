## gaussian splatting与nerf——新视角合成新范式
### 传统新视角合成  
最初的新视角合成方法基于光场技术，首先是密集采样，然后发展到允许非结构化捕获。运动结构恢复（SfM）的出现使得可以使用一系列照片来合成新视角，它在相机校准期间估计稀疏点云，最初用于3D空间的简单可视化。随后，多视图立体视觉（MVS）随着时间的推移产生了令人印象深刻的完整3D重建算法，促使多种视角合成算法的发展。这些方法将输入图像重新投影并融合到新视角相机中，并使用几何信息来指导这种重新投影。这些方法在许多情况下都取得了出色的结果，但通常无法从未重建区域或“过度重建”中完全恢复，即MVS生成不存在的几何结构。最近的神经渲染算法在很大程度上减少了这类伪影，并避免了在GPU上存储所有输入图像的巨大成本，从而在多方面超越了这些方法。

### ai新视角合成
早期的新视角合成技术采用了深度学习方法，如卷积神经网络（CNN）来估算融合权重或进行纹理空间的解决方案。然而，这些方法的一个主要缺点是依赖于基于多视图立体（MVS）的几何，常导致渲染时的时间闪烁问题。随后，为新视角合成引入了体积表示法，这类方法通过构建连续的可微分密度场来表示几何结构。然而，神经辐射场（NeRF）等方法在提高质量的同时，由于其庞大的多层感知机（MLP）结构，导致速度受到影响。最新的方法着重于通过使用空间数据结构、不同的编码方式和优化MLP容量来加快训练和/或渲染速度。尽管这些方法提供了杰出的结果，但在有效表示空间方面仍存在挑战，且图像质量在很大程度上受到用于加速的结构化网格的限制。而Gaussian Splatting使用的非结构化、显式的GPU友好的3D高斯方法实现了更快的渲染速度和更好的质量，无需神经网络组件。
  
Gaussian Splatting提出了以下贡献：
1. 引入各向异性3D高斯作为辐射场的高质量、非结构化表示。
2. 优化3D高斯属性的方法，结合自适应密度控制，创造高质量的捕获场景表示。
3. 一种快速的、可微分的GPU渲染方法，这种方法具有可见性感知、允许各向异性渲染和快速反向传播，以实现高质量的新视角合成。 在先前发布的数据集上的结果显示，可以从多视角捕获中优化的3D高斯，并达到等同或优于最佳隐式辐射场方法的质量。Gaussian splatting还实现了与最快方法相似的训练速度和质量，并且重要的是，提供了第一个实时渲染高质量新视角合成的方法。

## 单视角图片重建模型——zero123
最近，2D生成模型（如DALL-E、Imagen和Stable Diffusion）和视觉-语言模型（如CLIP）通过在大规模图像数据集上的预训练，学习了广泛的视觉概念。这些模型激发了一系列研究使用2D先验模型来辅助3D生成任务。一些工作（如DreamField、DreamFusion和Magic3D）遵循每个形状优化的范式，通常优化一个3D表示（如NeRF、网格、SMPL人体模型），并使用可微分渲染从不同视角生成2D图像。这些图像随后被送入CLIP模型或2D扩散模型来计算损失函数，用于指导3D形状优化。此外，一些工作在训练3D生成模型时利用CLIP的嵌入空间，一些工作则专注于使用2D模型的先验来生成输入网格的纹理或材料。在CLIP和大规模2D扩散模型出现之前，人们常从3D合成数据或真实扫描中学习3D先验。与2D图像不同，3D数据可以用不同格式表示，并且已经提出了许多特定于表示的3D生成模型。最近，越来越多的工作集中在从单个图像学习生成3D隐式场。一些最新的工作利用2D扩散模型进行每个形状的优化，使得文本到3D任务成为可能。为了从单个图像生成3D模型，一些工作利用文本反演来找到输入图像最匹配的文本嵌入，然后送入扩散模型。此外，OpenAI训练了一个3D原生扩散模型Point-E，用于生成点云，他们最近发布了另一个模型Shap-E，旨在生成隐函数的参数，用于生产纹理网格或神经辐射场。
重构单视角的3D物体是一个高度挑战性的问题，需要强大的先验知识。一些研究基于3D基元集合（如网格、体素或点云）构建先验知识，并使用图像编码器进行条件化。这些模型受到所用3D数据集多样性的限制，因其全局条件化特性而展现出较差的泛化能力。此外，它们需要额外的姿态估计步骤以确保估计的形状与输入对齐。另一方面，局部条件化模型直接使用局部图像特征进行场景重建，并展示出更强的跨域泛化能力，尽管通常限于近视图重构。最近，MCC学习了从RGB-D视图进行3D重建的通用表示，并在以物体为中心的大规模视频数据集上进行训练。在zero123的工作中，展示了可以直接从预训练的稳定扩散模型中提取丰富的几何信息，减少了对额外深度信息的需求。


zero123的主要贡献在于证明大型扩散模型已经学习了关于视觉世界的丰富3D先验知识，即使它们只在2D图像上进行训练。还展示了在从单个RGB图像进行新视角合成和零样本3D物体重建方面的最新成果。文中首先简要回顾了相关工作，然后描述了通过微调大型扩散模型来学习相机外参控制的方法。最后，展示了多项定量和定性实验来评估从单张图像进行零样本视角合成和3D几何与外观重建。将发布所有代码、模型及在线演示。

## 3d editting 
编辑神经场由于其形状和外观之间的复杂相互作用而具有挑战性。EditNeRF是这一领域的先驱作品，它通过对潜在代码进行条件化来编辑神经场的形状和颜色。此外，一些研究利用CLIP模型通过文本提示或参考图像来促进编辑。另一方面的研究聚焦于使用预定义的模板模型或骨架来支持特定类别内的重新定位或重新渲染。基于几何的方法将神经场转换为网格，并与隐式场同步网格变形。此外，3D编辑技术涉及将2D图像操作（如修复）与神经场训练结合起来。同时进行的工作利用静态2D和3D遮罩来限制NeRF的编辑区域。然而，这些方法有其局限性，因为3D模型的训练是一个动态过程，静态遮罩无法有效地限制它。相比之下，我们的研究采用高斯语义追踪在整个训练过程中追踪目标高斯。
（来自gaussian editor）

## 动态3维重建
神经渲染技术结合了机器学习和几何推理，已成为从稀疏图像集合合成场景新视角的最有前景的方法之一。其中，神经辐射场（NeRF）尤为突出，它训练深度网络将5D输入坐标（代表空间位置和观察方向）映射为体积密度和视角依赖的辐射亮度。然而，尽管NeRF在生成图像的真实感上达到了前所未有的水平，但它只适用于静态场景。在这篇论文中，介绍了D-NeRF，这是一种将神经辐射场扩展到动态领域的方法，允许从围绕场景移动的单个摄像机重建和渲染对象在刚性和非刚性运动下的新图像。为此，将时间作为系统的额外输入，并将学习过程分为两个主要阶段：一个编码场景到规范空间，另一个将这个规范表示映射到特定时间的变形场景。这两种映射使用全连接网络同时学习。一旦网络训练完成，D-NeRF可以渲染新图像，控制相机视角和时间变量，从而控制物体运动。在刚性、关节和非刚性运动下的场景中证明了方法的有效性。代码、模型权重和动态场景数据集将发布。
为许多应用（如增强现实、虚拟现实、3D内容制作、游戏和电影产业）从稀疏的图像集合合成逼真的新视角是必要的。近期在神经渲染领域的进展，通过学习同时编码几何和外观的场景表示，已经大大超越了传统的结构从运动、光场摄影和基于图像的渲染方法。例如，神经辐射场（NeRF）表明简单的多层感知机网络可以编码从5D输入（表示空间位置和相机视角）到辐射值和体积密度的映射，从而实现极其逼真的自由视点渲染。然而，所有这些方法都假设场景是静态的，没有移动物体。在本文中，打破这个假设，提出了第一个适用于动态场景的端到端神经渲染系统。的方法将系统输入表示为一个连续的6D函数，包括3D位置、相机视角和时间成分。的观察是，物体可以移动和变形，但通常不会出现或消失。的方法，动态神经辐射场（D-NeRF），分解学习为两个模块，一个学习空间映射，另一个回归场景辐射和体积密度。学习的模型允许合成新图像，同时控制相机视角和时间成分，或者等效地，场景的动态状态。在不同类型的变形场景上进行了全面评估，结果表明D-NeRF能够在控制相机视角和时间成分的同时渲染高质量图像。的方法还能产生捕捉时变几何形状的完整3D网格。
（来自D-nerf）
