## gaussian splatting与nerf——新视角合成新范式


传统新视角合成
最初的新视角合成方法基于光场技术，起初是密集采样，后来允许非结构化捕获。结构从运动（SfM）的出现使得使用一系列照片来合成新视角成为可能。SfM在相机校准期间估计一个稀疏点云，最初用于简单的3D空间可视化。随后的多视图立体视觉（MVS）在多年间产生了令人印象深刻的完整3D重建算法，促进了多种视角合成算法的发展。所有这些方法都将输入图像重新投影并混合到新视角相机中，并使用几何信息来引导这种重新投影。这些方法在许多情况下都取得了优秀的结果，但通常无法完全从未重建区域或“过度重建”中恢复，即MVS生成不存在的几何结构。最新的神经渲染算法大大减少了这类伪影，并避免了在GPU上存储所有输入图像的巨大成本，在大多数方面都优于这些方法。



ai新视角合成
最初，卷积神经网络（CNNs）被用于估计混合权重或处理纹理空间解决方案。这些方法的主要缺点是依赖基于多视图立体视觉（MVS）的几何结构，且使用CNN进行最终渲染常导致时间上的闪烁。之后，提出了结合深度学习技术的体积表示方法，以连续可微的密度场表示几何结构。然而，传统的体积光线追踪渲染方式由于需要查询大量样本而成本较高。NeRF通过引入重要性采样和位置编码来提高质量，但使用大型多层感知机（MLP），影响了速度。最新的方法着重于通过使用空间数据结构、不同编码和MLP容量来加快训练和/或渲染速度。这些方法包括空间离散化、代码本和编码（如哈希表），允许使用更小的MLP或完全不使用神经网络。最值得注意的方法包括InstantNGP和Plenoxels，它们分别使用哈希网格和稀疏体素网格加速计算，并能够不使用神经网络。尽管这些方法取得了杰出的成果，但在有效表示空间方面仍存在挑战，并且图像质量在很大程度上受到用于加速的结构化网格的选择限制，渲染速度也受到光线追踪步骤中需要查询多个样本的影响。而作者使用的非结构化、显式的GPU友好的3D高斯方法实现了更快的渲染速度和更好的质量，而无需神经组件。
（上面两个来自于gaussian splatting 2.1 2.2 2.3）



image-2-3d
最近，2D生成模型（如DALL-E、Imagen和Stable Diffusion）和视觉-语言模型（如CLIP）通过在大规模图像数据集上的预训练，学习了广泛的视觉概念。这些模型激发了一系列研究使用2D先验模型来辅助3D生成任务。一些工作（如DreamField、DreamFusion和Magic3D）遵循每个形状优化的范式，通常优化一个3D表示（如NeRF、网格、SMPL人体模型），并使用可微分渲染从不同视角生成2D图像。这些图像随后被送入CLIP模型或2D扩散模型来计算损失函数，用于指导3D形状优化。此外，一些工作在训练3D生成模型时利用CLIP的嵌入空间，一些工作则专注于使用2D模型的先验来生成输入网格的纹理或材料。在CLIP和大规模2D扩散模型出现之前，人们常从3D合成数据或真实扫描中学习3D先验。与2D图像不同，3D数据可以用不同格式表示，并且已经提出了许多特定于表示的3D生成模型。最近，越来越多的工作集中在从单个图像学习生成3D隐式场。一些最新的工作利用2D扩散模型进行每个形状的优化，使得文本到3D任务成为可能。为了从单个图像生成3D模型，一些工作利用文本反演来找到输入图像最匹配的文本嵌入，然后送入扩散模型。此外，OpenAI训练了一个3D原生扩散模型Point-E，用于生成点云，他们最近发布了另一个模型Shap-E，旨在生成隐函数的参数，用于生产纹理网格或神经辐射场。


（来自zero 123）

text-2-gaussian
文本到3D生成旨在从文本提示生成3D素材。最近，基于数据驱动的2D扩散模型在文本到图像生成方面取得了显著成就。然而，将其转移到3D生成上并非易事，因为策划大规模3D数据集具有挑战性。现有的3D原生扩散模型通常只针对单一物体类别，且多样性有限。为了实现开放词汇的3D生成，一些方法提出将2D图像模型用于3D生成。这些2D提升方法通过优化3D表示，以确保在不同视角渲染时在预训练的2D扩散模型中获得高概率，从而确保3D一致性和真实感。后续工作继续增强生成保真度和训练稳定性，并探索进一步应用。然而，这些基于优化的2D提升方法通常存在长时间的逐例优化问题。特别是，使用NeRF作为3D表示会在前向和后向过程中导致昂贵的计算。在这项工作中，我们选择3D高斯作为可微分的3D表示，并从经验上展示它具有更简单的优化景观。
（来自dream gaussian）


3d editting 
编辑神经场由于其形状和外观之间的复杂相互作用而具有挑战性。EditNeRF 在这一领域开创了先河，它通过对神经场使用潜在代码来编辑形状和颜色。此外，一些工作利用CLIP模型通过文本提示或参考图像来便于编辑。另一方面的研究聚焦于使用预定义的模板模型或骨架来支持特定类别内的重新定位或重新渲染。基于几何的方法将神经场转换为网格，并与隐式场同步网格变形。此外，3D编辑技术涉及将2D图像操作（如修复）与神经场训练结合起来。同时，一些工作利用静态2D和3D遮罩来限制NeRF的编辑区域。然而，这些方法有其局限性，因为3D模型的训练是一个动态过程，静态遮罩无法有效地限制它。与此相反，我们的研究采用高斯语义追踪在整个训练过程中追踪目标高斯。
（来自gaussian editor）

动态3维重建
神经渲染技术结合了机器学习和几何推理，已成为从稀疏图像集合合成场景新视角的最有前景的方法之一。其中，神经辐射场（NeRF）尤为突出，它训练深度网络将5D输入坐标（代表空间位置和观察方向）映射为体积密度和视角依赖的辐射亮度。然而，尽管NeRF在生成图像的真实感上达到了前所未有的水平，但它只适用于静态场景。在这篇论文中，我们介绍了D-NeRF，这是一种将神经辐射场扩展到动态领域的方法，允许从围绕场景移动的单个摄像机重建和渲染对象在刚性和非刚性运动下的新图像。为此，我们将时间作为系统的额外输入，并将学习过程分为两个主要阶段：一个编码场景到规范空间，另一个将这个规范表示映射到特定时间的变形场景。这两种映射使用全连接网络同时学习。一旦网络训练完成，D-NeRF可以渲染新图像，控制相机视角和时间变量，从而控制物体运动。我们在刚性、关节和非刚性运动下的场景中证明了我们方法的有效性。代码、模型权重和动态场景数据集将发布。
为许多应用（如增强现实、虚拟现实、3D内容制作、游戏和电影产业）从稀疏的图像集合合成逼真的新视角是必要的。近期在神经渲染领域的进展，通过学习同时编码几何和外观的场景表示，已经大大超越了传统的结构从运动、光场摄影和基于图像的渲染方法。例如，神经辐射场（NeRF）表明简单的多层感知机网络可以编码从5D输入（表示空间位置和相机视角）到辐射值和体积密度的映射，从而实现极其逼真的自由视点渲染。然而，所有这些方法都假设场景是静态的，没有移动物体。在本文中，我们打破这个假设，提出了第一个适用于动态场景的端到端神经渲染系统。我们的方法将系统输入表示为一个连续的6D函数，包括3D位置、相机视角和时间成分。我们的观察是，物体可以移动和变形，但通常不会出现或消失。我们的方法，动态神经辐射场（D-NeRF），分解学习为两个模块，一个学习空间映射，另一个回归场景辐射和体积密度。学习的模型允许合成新图像，同时控制相机视角和时间成分，或者等效地，场景的动态状态。我们在不同类型的变形场景上进行了全面评估，结果表明D-NeRF能够在控制相机视角和时间成分的同时渲染高质量图像。我们的方法还能产生捕捉时变几何形状的完整3D网格。
（来自D-nerf）
