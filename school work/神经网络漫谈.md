## **引言**

### **神经网络可以拿来干什么？**

他可以用于逼近任意函数，可以用于替代传统方法中的算法，也可以用于探寻传统方法不能解决的问题。具体来说， 万能近似定理（universal approximation theorem）(Hornik et al., 1989;Cybenko, 1989) 表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel 可测函数。

### **神经网络如何学习，以监督式训练为例**

#### **反向传播与adam损失器**

## **CNN——从传统视觉说起**

### 我们如何让机器理解图片——二维傅立叶变换
#### 一维傅立叶变换
##### 有损压缩理解

归根到底就是把函数用一组三角函数基函数来拟合，选取这组的原因我们先暂且不表。
$$
\begin{aligned}
& y_0=1 \\
& y_1=\sin (x) \\
& y_2=\sin (2 x) \\
& y_3=\sin (4 x)
\end{aligned}
$$


在高数上中我们学习过泰勒展开，而他则是使用多项式基函数。
$$
\begin{aligned}
& y_0=1 \\
& y_1=x \\
& y_2=x^2 \\
& y_3=x^3
\end{aligned}
$$


或者也可以随便乱写一个基函数：
$$
\begin{aligned}
& y_0=555 \\
& y_1=\frac{1}{x}+\tan (x) \\
& y_2=x^3-666 \\
& y_3=\sin (4 x)
\end{aligned}
$$

有了基函数, 就可以把任意一个函数, 描述成几个基函数的加权和了。

例如
$$
y \approx 0.1 y_0+0.3 y_1+0.8 y_2+0.001 y_3+\ldots
$$

这时候, 就相当于是把一个原始函数
$$
y=f(x)
$$

变成了一组系数:
$$
0.1,0.3,0.8,0.001, \ldots
$$

一般的, 能用的基函数个数越多, 表达能力就越强。本质上是一个有损压缩。有点像个密码本，你一本我一本，上面写了基函数的定义，这样传密码的时候只要传几个系数就可以了，系数传到我这儿，我能复原出y = f(x)，只是没那么准确了。

![[Pasted image 20240105103041.png]]
![[Pasted image 20240105103644.png]]
具有线性性质，对组合函数进行傅立叶变换=分别进行傅立叶变换后相加


##### 从几何角度考虑公式含义

$e^{-2\pi ift}$代表了复平面内频率为f的旋转
![[Pasted image 20240105104516.png]]
如果将其点乘上信号强度关于时间的函数，就可以看成把这个函数按照频率f缠绕在圆上
![[Pasted image 20240105104751.png]]
$\int_{t_1}^{t_2}g(t)e^{-2\pi ift}$则代表了缠绕频率f的情况下按照$t_2-t_1$倍增后的质心偏离圆点距离。于是我们可以得到$\hat g(f)=\int_{t_1}^{t_2}g(t)e^{-2\pi ift}$这样就可以把函数从
![[Pasted image 20240105105234.png]]
##### 从信号角度分析
公式: $F(w)=\int_{-\infty}^{+\infty} f(x) e^{-j w x} d x$

通俗来讲, 一维傅里叶变换是将一个一维的信号分解成若干个复指数波 $e^{j w x}$ 。而由于 $e^{j w x}=\cos (w x)+j \sin (w x)$, 所以可以将每一个复指数波 $e^{j w x}$ 都视为是余弦波 $+\mathrm{j}^*$ 正弦波的组合。

对于一个正弦波而言, 需要三个参数来确定它: 频率 $w$, 幅度 $A$, 相位 $\varphi$ 。因此在频域中, 一维坐标代表频率, 而每个坐标对应的函数值也就是 $F(w)$ 是一个复数, 其中它的幅度 $|F(w)|$ 就是这个频率正弦波的幅度 $A$, 相位 $\angle F(w)$ 就是 $\varphi$ 。

一维傅里叶变换就是一个基变换, 在时域中, 基是一族冲激信号 $\{\delta(x-n)\}$ ，在频域中; 基是 $\left\{e^{j w x}\right\}$ ，而且这组基是正交基。

![[Pasted image 20240105110643.png]]
#### 二维傅立叶变换
一维信号是一个序列, FT将其分解成若干个一维的简单函数之和。二维的信号可以说是一个图像,类比一维, 那二维 $\mathrm{FT}$ 是将一个图像分解成若干个简单的图像（即复平面波 $e^{j 2 \pi(u x+v y)}$ 之和）。如下图:
![[Pasted image 20240105110923.png]]

- 二维FT的公式: $F(u, v)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x, y) e^{-j 2 \pi(u x+v y)} d x d y$

从公式也可以看到, 二维傅里叶变换就是将图像与每个不同频率的不同方向的复平面波做内积（先点乘再求和），也就是一个求在基 $\left\{e^{-j 2 \pi(u x+v y)}\right\}$ 上的投影的过程。
而二维正弦波的确定需要四个参数: 频率 $w$, 幅度 $A$, 相位 $\varphi$ 以及法向量$\vec n$
![[Pasted image 20240105111727.png]]
于是我们可以以u，v为坐标轴，构建一个二维空间，即k空间（一般用k来表示空间频率，单位是 $1 / \mathrm{m}$ )
因为向量类似于复数，它是有方向的, 也是有长度的。就是说一个点 $(u, v)$ 代表这个平面波的法向量 $\vec{n}$, 这个向量的模 $\sqrt{u^2+v^2}$代表这个平面波的频率 $w$ ，这个点里面保存的内容复数就是此平面波的幅度和相位。也因此K空间的中心对于低频，周围对于高频。下面这个图很好的体现了这一点:
![[Pasted image 20240105112800.png]]
再如下面这个图片, 中心低频贡献了图像的主体, 周围高频提供图像的细节和边缘。
![[Pasted image 20240105113322.png]]
因此, $k$ 空间的每一个位置存储的数代表了所在位置复平面波在图像中占多少成分, 我们就可以用每个系数*所代表的平面波相加得到原来的图像, 也就是下图。所以 $k$ 空间和对应图像储存的信息含量是一样的，只不过表现形式不同，或者说基不同。
![[Pasted image 20240105113514.png]]

### 让机器加工图片——滤波器

![](file:////private/var/folders/d7/6dvz56_15_b58n3vh_m10hhh0000gn/T/com.kingsoft.wpsoffice.mac/wps-lasdxzz/ksohtml//wps1.jpg) 

resnet

unet




## **nerf——渲染的ai形式**

计算机上如何显示图片——栅格渲染

无法避免的代价——频率混叠与采样定理

光与影的世界——光线追踪

3维储存结构

nerf原理

## **gaussian splatting——基于点云的ai渲染新范式**

MVS——如何从多视角视图中重建点云

gaussian splatting原理

## **transformer——attention is all your need**

注意力机制的发展史

位置编码，以及给diffusion带来的时间步编码

transformer的意义

bert语言大模型

vilt多模态大模型

## **diffusion——生成式模型的骄子**

生成式模型的发展历史

diffusion发展史

对比训练

数据增强

zero-shot能力

单视角图片重建

根据文本生成——图片、视频、语音