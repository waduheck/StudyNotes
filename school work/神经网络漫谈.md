## **引言**

### **神经网络可以拿来干什么？**

他可以用于逼近任意函数，可以用于替代传统方法中的算法，也可以用于探寻传统方法不能解决的问题。具体来说， 万能近似定理（universal approximation theorem）(Hornik et al., 1989;Cybenko, 1989) 表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel 可测函数。

### **神经网络如何学习，以监督式训练为例**

#### **反向传播与adam损失器**

## **CNN——从传统视觉说起**

### 我们如何让机器理解图片——二维傅立叶变换
#### 一维傅立叶变换
归根到底就是把函数用一组三角函数基函数来拟合，选取这组的原因我们先暂且不表。
$$
\begin{aligned}
& y_0=1 \\
& y_1=\sin (x) \\
& y_2=\sin (2 x) \\
& y_3=\sin (4 x)
\end{aligned}
$$


在高数上中我们学习过泰勒展开，而他则是使用多项式基函数。
$$
\begin{aligned}
& y_0=1 \\
& y_1=x \\
& y_2=x^2 \\
& y_3=x^3
\end{aligned}
$$


或者也可以随便乱写一个基函数：
$$
\begin{aligned}
& y_0=555 \\
& y_1=\frac{1}{x}+\tan (x) \\
& y_2=x^3-666 \\
& y_3=\sin (4 x)
\end{aligned}
$$

有了基函数, 就可以把任意一个函数, 描述成几个基函数的加权和了。

例如
$$
y \approx 0.1 y_0+0.3 y_1+0.8 y_2+0.001 y_3+\ldots
$$

这时候, 就相当于是把一个原始函数
$$
y=f(x)
$$

变成了一组系数:
$$
0.1,0.3,0.8,0.001, \ldots
$$

一般的, 能用的基函数个数越多, 表达能力就越强。本质上是一个有损压缩。有点像个密码本，你一本我一本，上面写了基函数的定义，这样传密码的时候只要传几个系数就可以了，系数传到我这儿，我能复原出y = f(x)，只是没那么准确了。

![[Pasted image 20240105103041.png]]


让机器加工图片——滤波器

![](file:////private/var/folders/d7/6dvz56_15_b58n3vh_m10hhh0000gn/T/com.kingsoft.wpsoffice.mac/wps-lasdxzz/ksohtml//wps1.jpg) 

resnet

unet

## **nerf——渲染的ai形式**

计算机上如何显示图片——栅格渲染

无法避免的代价——频率混叠与采样定理

光与影的世界——光线追踪

3维储存结构

nerf原理

## **gaussian splatting——基于点云的ai渲染新范式**

MVS——如何从多视角视图中重建点云

gaussian splatting原理

## **transformer——attention is all your need**

注意力机制的发展史

位置编码，以及给diffusion带来的时间步编码

transformer的意义

bert语言大模型

vilt多模态大模型

### **diffusion——生成式模型的骄子**

生成式模型的发展历史

diffusion发展史

对比训练

数据增强

zero-shot能力

单视角图片重建

根据文本生成——图片、视频、语音