BERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非想Bi-LSTM那样把句子倒序输入一遍。
https://www.jiqizhixin.com/articles/2019-11-05-2
https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/bert.html

但是作者认为其并未充分利用句子的语言结构。利用语言模型在一系列的词语和句子找到最佳排列是NLP任务的本质，例如机器翻译，基于此，作者提出了一种新型的上下文表示模型，StructBERT。StrucBERT分别利用在句子中和句子间增加结构信息，词序(word-level ordering)和句序(sentence-level ordering)来增强模型的预训练，这样在预训练上可以明确捕获语言方面的内容，可以在上下文表示中对句子和单词之间的依存关系进行编码表示，增强了模型的通用性和适用性。
### 视频演示文字稿：数据标注平台 — 定义数据未来的关键一步

#### 引言
- **旁白**：“在这个数据驱动的时代，高质量的数据标注对于机器学习和人工智能至关重要。今天，我们将向您展示我们的数据标注平台，它不仅提供工具，还是数据科学家和工程师的有力支持，帮助他们提高数据处理效率和准确性。”

#### 平台概览
- **场景一**：平台登录界面展示。
  - **旁白**：“首先，让我们从简洁直观的登录界面开始探索。这里用户可以轻松注册和登录，进入数据标注的世界。”

- **场景二**：展示项目管理界面。
  - **旁白**：“在项目管理界面，用户可以创建、查看和管理数据标注项目，确保每个项目都在有序进行。”

- **场景三**：标注界面展示。
  - **旁白**：“标注界面是平台的核心，提供了丰富的功能，比如信息标注、文本分类、图像文本标注和图片分类等，满足不同类型数据的标注需求。”

#### 技术介绍
- **场景四**：展示Struct BERT的应用实例。
  - **旁白**：“利用Struct BERT，我们的平台可以高效处理和理解大规模文本数据，特别是在精确信息抽取和文本分类方面。”

- **场景五**：展示Seglink++在工作中的样子。
  - **旁白**：“Seglink++能够精准地识别和标注图像中的文本，即使在复杂的街景和广告牌中也同样表现出色。”

- **场景六**：VIT模型的展示。
  - **旁白**：“VIT模型将传统的卷积网络与Transformer结构相结合，提供了对图片全局理解的强大能力，特别适合大规模和复杂的图像分类任务。”

#### 结尾
- **场景七**：展示平台的高级功能和个性化设置。
  - **旁白**：“通过我们的数据标注平台，您可以高效管理和精确标注各类数据。无论您所在的行业领域如何，我们的平台都将为您提供可靠的数据标注解决方案。”

- **尾声**：
  - **旁白**：“感谢您的观看。我们的数据标注平台是您在数据驱动时代的强大伙伴。一起探索数据的无限可能性，共创美好未来！”

#### 结束画面
- **旁白**：“访问我们的网站了解更多，或联系我们的团队进行咨询。”（展示网站链接和联系信息）